{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Before starting, read the README.txt file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Simple Linear Regression  (30 points)\n",
    "\n",
    "### Background\n",
    "\n",
    "Remember from the course material: In Linear Regression, we aim to find the line that fits the given data as well as possible. The line formula is:\n",
    "\n",
    "### <center> $ \\hat{y} = f(x) = \\beta_0 + \\beta_1 x $ <center> \n",
    "\n",
    "And using linear regression, the goal is to find the $\\beta_0$ and $\\beta_1$ such that the the Mean Squared Error (MSE) is minimized:\n",
    "    \n",
    "### <center> $ MSE \\Rightarrow \\mathcal{L} = \\dfrac{1}{N} \\sum_{i=1}^{N} (y_i - f(x_i ))^2 $ <center> \n",
    "    \n",
    "As the MSE error gets smaller, the line starts to represent the data better and better.\n",
    "    \n",
    "Consider $\\beta = (\\beta_0 + \\beta_1)$. In order to find the $\\beta$ that minimizes the MSE loss, we use **Gradient Descent**. Iteratively $\\beta$ is updated in the opposite direction of the gradient, while the size of the update is controlled with the learning rate $\\eta$:\n",
    "    \n",
    "### <center> $ \\beta \\leftarrow \\beta-\\eta \\frac{d \\mathcal{L}}{d \\beta} $ <center> \n",
    "\n",
    "### Your Task\n",
    "\n",
    "In this part, you are going to code up Simple Linear Regression on the data given in **data1.csv**, and find the line that fits the data best. To do so, you'll need to complete the following:\n",
    "\n",
    "- Code up the gradient descent, and print the loss value at every time step. You can use the number of steps as the stopping criteria (e.g. stopping gradient descent after 10 steps). \n",
    "- At the end of the algorithm, plot the obtained regression line with the data .\n",
    "    \n",
    "I recommend you to use the given learning rate and number of iterations in the code , but you are free to change them to make your algorithm faster if you'd like. You are not allowed to use any libraries other than the imported ones at the beginning. You can initialize the $\\beta$ randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv(\"data1.csv\")\n",
    "    x = df['x'].values\n",
    "    y = df['y'].values\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, learning_rate = 0.1, max_its = 500):\n",
    "    #TO DO: Initialize the beta values\n",
    "    beta0 = 0.0\n",
    "    beta1 = 0.0\n",
    "    \n",
    "    m = len(y)\n",
    "\n",
    "    # start gradient descent loop\n",
    "    for k in range(0,max_its):\n",
    "        predictions = beta0 + beta1 * x\n",
    "        \n",
    "        error = predictions - y\n",
    "        # TO DO: calculate derivative (grad)\n",
    "        gradient0 = np.sum(error)/m\n",
    "        gradient1 = np.sum(error * x)/m\n",
    "        \n",
    "        \n",
    "        # TO DO: take gradient descent step\n",
    "        beta0 -= learning_rate * gradient0\n",
    "        beta1 -= learning_rate *gradient1\n",
    "        \n",
    "        \n",
    "        # TO DO: calculate and print the loss value\n",
    "        loss = np.sum(error**2)/(m)\n",
    "        print(f\"iteration {k + 1}/{max_its}, Loss: {loss}\")\n",
    "\n",
    "    return beta0, beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[0;32m      2\u001b[0m beta0, beta1 \u001b[38;5;241m=\u001b[39m gradient_descent(x, y)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(x, y)\n",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     x \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m     y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = load_data()\n",
    "beta0, beta1 = gradient_descent(x, y)\n",
    "plt.scatter(x, y)\n",
    "axes = plt.gca()\n",
    "x_vals = np.array(axes.get_xlim())\n",
    "y_vals = beta0 + beta1 * x_vals\n",
    "plt.plot(x_vals, y_vals, '--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multi Linear Regression on California House Pricing Dataset using 5-fold Cross Validation (50 points)\n",
    "\n",
    "In this part, you are going to train a Multi Linear Regression Model on a real dataset! The dataset we are going to use is **California House Pricing Dataset**. The target is to predict the median house value in California, given the 8 features ( You can examine the name of the features when you run the load_data function given below ). In this part, you will also use Cross-Validation with 5 folds, in addition to everything you implemented in Part 1. You will also alter your code so that it works with multiple attribute. In this case, you will have 8 features and you can code up assuming you will always have 8 features. At the end, you will report your results. \n",
    "\n",
    "In summary, you will need to do the following:\n",
    "\n",
    "- Code up gradient descent with Cross Validation for Multi Linear Regression\n",
    "- Find the best working learning rate and number of iterations setup.\n",
    "- Report the MSE loss periodically during training in folds (For example, if you are training for 1000 steps, you can print MSE loss for each 100 steps. If you are training for 100000 steps ( which you can, if you'd like ), you can print MSE loss for each 10000 steps. In total, if you print the loss 10 times periodically for each fold, it's fine ).\n",
    "- Save the final MSE results on the testing set on each fold in an array. Print the average testing MSE losses at the end of the algorithm. Additionally, print the variance of the testing MSE losses. \n",
    "\n",
    "In this part, I am giving you only the load_data function, It is up to you to find the working learning rate and number of iterations setup. In addition to the libraries imported for Part 1, you are allowed to use sklearn for fetching dataset (which is already done for you), and the KFold class for the cross-validation. Other than that, you are not allowed to use sklearn or any other additional libraries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3997406013.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[54], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    def multiple_gradient_descent_KFold(...):\u001b[0m\n\u001b[1;37m                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import KFold\n",
    "def load_data():\n",
    "    data= fetch_california_housing()\n",
    "    print(\"Features of Boston Housing Prices dataset: \", data.get(\"feature_names\"))\n",
    "    print(\"Shape of the X: \", data.data.shape)\n",
    "    print(\"Shape of the Y: \", data.target.shape)\n",
    "    return data.data, data.target\n",
    "\n",
    "\n",
    "\n",
    "def multiple_gradient_descent_KFold(...):\n",
    "   \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features of Boston Housing Prices dataset:  ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
      "Shape of the X:  (20640, 8)\n",
      "Shape of the Y:  (20640,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'multiple_gradient_descent_KFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x, y \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m----> 2\u001b[0m multiple_gradient_descent_KFold(x, y)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'multiple_gradient_descent_KFold' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = load_data()\n",
    "multiple_gradient_descent_KFold(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
